<!DOCTYPE html>
<html>
  <head>
    <title>Artifact Evaluation: Motivation</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet" media="screen">
  </head>
  <body>
    <script src="http://code.jquery.com/jquery.js"></script>
    <script src="js/bootstrap.min.js"></script>

<div class="container">

  <div class="hero-unit">

    <table>
      <tr>
        <td width="60%">

<h2>Motivation</h2>

        </td>
        <td>
<img src="img/pistons.JPG">
	</td>
      </tr>
    </table>

  </div>

<h3>What is It and Why do We Need It?</h3>

<p>                                                                             
For many years, some of us in programming languages and software                
engineering have been concerned by the insufficient respect paid to             
the artifacts that back papers.  We find it especially ironic that              
areas that are so centered on software, models, and specifications              
would not want to evaluate them as part of the paper review process,            
as well as archive them with the final paper.  Not examining artifacts          
enables everything from mere sloppiness to, in extreme cases,                   
dishonesty.  More subtly, it also imposes a subtle penalty on people            
who take the trouble to vigorously implement and test their ideas.              
</p>

<p>                                                                             
In 2011, Andreas Zeller, the program chair for ESEC/FSE, decided to             
institute a committee to address this problem.  Andreas asked Carlo             
Ghezzi and Shriram Krishnamurthi to run this process.
</p>

<blockquote>                                                                             
An aside on naming. Shriram had long wanted to create such a committee and          
call it the &ldquo;Program Committee&rdquo; (ha, ha).  However, not             
only is that name taken, we also wanted to be open-minded to all sorts          
of artifacts that are not programs (not only models but also data               
sets, etc.).  We therefore called this the Artifact Evaluation                  
Committee (AEC).  We hoped that someone would come up with a better name for            
this eventually, but that has yet to happen while this name seems to
  be increasingly entrenched.
</blockquote>

<h3>Design Criteria</h3>

<p>                                                                             
For several months before the deadline, Carlo and Shriram consulted with several               
software engineering community leaders about the wisdom of having an            
AEC.  Most responded positively; a few were tepid; a small number were          
negative and gave constructive feedback.  Here are some of the most          
prominent issues that people raised:
</p>

<ul>

<li>                                                                            
Introducing this new step into the evaluation process for papers                
might be unfair to authors.                                                     
</li>

<li>                                                                            
It's expensive to create packaged artifacts, and the value in doing             
so is not clear.                                                                
</li>

<li>                                                                            
Exposing artifacts to the general public before extracting enough               
value for the group that created them is unfair to that group.                  
</li>

<li>                                                                            
Industrial researchers (but not only them) might not be willing to              
share their artifacts due to various proprietary considerations.                
</li>

<li>                                                                            
Fear of failing to meet expectations may lead authors to simply not             
submit artifacts or, worse, skip the conference.                                
</li>

</ul>

<p>                                                                             
It became clear that there was a strong desire to be conservative in            
the design of this process, at least initially. They therefore decided           
that, in addition to treating artifacts with the same confidentiality           
rules as papers (as we had always intended), artifacts did not need to be            
made public: it was sufficient if only the AEC saw them.  (Obviously,           
they encouraged authors to upload them to the supplement section of the           
ACM DL and/or make them public on their own sites.)                             
</p>

<p>                                                                             
They also made two especially important decisions:                                
</p>

<ol>

<li>                                                                            
They erected a Chinese Wall between the paper and artifact                 
evaluation processes: that is, the outcome of artifact evaluation               
would have no bearing at all on the paper's decision.  The simplest             
way to assure the public of this was via temporal ordering: that is,            
artifact submission and evaluation began only after paper decisions             
had been published.  Confident authors could, of course, provide                
artifact links in their papers, as some already do, but they were not           
required to do so.                                                              
</li>

<li>                                                                            
The outcome of artifact evaluation for individual papers would be               
reported by the authors, who had a choice of suppressing          
this information in case of a negative review.                                  
</li>

</ol>

<p>                                                                             
These decisions seemed to reassure several people that the process              
would truly be a conservative extension of current review mechanisms,           
and would not adversely affect the conference. These design elements
have persisted through several runnings of the AEC for numerous
conferences, but they should be understood in their historical
context&mdash;as a means of addressing the desire for a
strictly conservative extension to existing processes&mdash;than as
driven by wanting particular outcomes for artifacts.
</p>

<div>

  </body>
</html>
